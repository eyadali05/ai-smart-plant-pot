{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f532e75",
   "metadata": {},
   "source": [
    "# ðŸŒ± Plant Classifier â€” Fixed & Fine-Tuned Version\n",
    "This notebook trains and fine-tunes a MobileNetV3Small model to classify house plants, then exports an INT8-quantized TFLite model ready for Raspberry Pi deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*RGBA.*\")\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "processed_root = ROOT / \"data_processed\"\n",
    "models_dir = ROOT / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "classes = sorted([p.name for p in processed_root.iterdir() if p.is_dir()])\n",
    "num_classes = len(classes)\n",
    "print(\"âœ… Classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c87d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Augmentation ----\n",
    "def augment_numpy(image_np, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    img = Image.fromarray(image_np)\n",
    "    w, h = img.size\n",
    "    scale = rng.uniform(0.9, 1.0)\n",
    "    new_w, new_h = int(w*scale), int(h*scale)\n",
    "    left = rng.integers(0, max(1, w-new_w))\n",
    "    top  = rng.integers(0, max(1, h-new_h))\n",
    "    img = img.crop((left, top, left+new_w, top+new_h)).resize((IMG_SIZE, IMG_SIZE))\n",
    "    img = ImageEnhance.Brightness(img).enhance(rng.uniform(0.8, 1.2))\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    if rng.random() < 0.4:\n",
    "        arr += rng.normal(0, 10, arr.shape)\n",
    "    arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c95e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Data Loaders ----\n",
    "def load_and_augment_train(path, label):\n",
    "    def _load_and_aug(p, lbl):\n",
    "        try:\n",
    "            img = Image.open(p.decode()).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "            arr = np.array(img)\n",
    "            arr = augment_numpy(arr)\n",
    "            return arr.astype(np.float32)/255.0, np.int32(lbl)\n",
    "        except Exception:\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32), -1\n",
    "    img, lbl = tf.py_function(_load_and_aug, [path, label], [tf.float32, tf.int32])\n",
    "    img.set_shape([IMG_SIZE, IMG_SIZE, 3])\n",
    "    lbl.set_shape([])\n",
    "    return img, lbl\n",
    "\n",
    "def load_image_simple(path, label):\n",
    "    def _load(p, lbl):\n",
    "        try:\n",
    "            img = Image.open(p.decode()).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "            return np.array(img).astype(np.float32)/255.0, np.int32(lbl)\n",
    "        except Exception:\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32), -1\n",
    "    img, lbl = tf.py_function(_load, [path, label], [tf.float32, tf.int32])\n",
    "    img.set_shape([IMG_SIZE, IMG_SIZE, 3])\n",
    "    lbl.set_shape([])\n",
    "    return img, lbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937742c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Dataset ----\n",
    "def get_paths_and_labels(split):\n",
    "    paths, labels = [], []\n",
    "    for idx, cls in enumerate(classes):\n",
    "        for f in (processed_root/cls/split).glob(\"*\"):\n",
    "            if f.is_file(): paths.append(str(f)); labels.append(idx)\n",
    "    return paths, labels\n",
    "\n",
    "train_paths, train_labels = get_paths_and_labels(\"train\")\n",
    "val_paths, val_labels = get_paths_and_labels(\"val\")\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "    .shuffle(1000)\n",
    "    .map(load_and_augment_train, num_parallel_calls=AUTOTUNE)\n",
    "    .filter(lambda img, lbl: tf.not_equal(lbl, -1))\n",
    "    .batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    ")\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "    .map(load_image_simple, num_parallel_calls=AUTOTUNE)\n",
    "    .filter(lambda img, lbl: tf.not_equal(lbl, -1))\n",
    "    .batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    ")\n",
    "print(\"âœ… Dataset built successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Model ----\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "base = tf.keras.applications.MobileNetV3Small(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights=\"imagenet\"\n",
    ")\n",
    "base.trainable = False\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = tf.keras.applications.mobilenet_v3.preprocess_input(inputs)\n",
    "x = base(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Training ----\n",
    "EPOCHS = 10\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e194793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# ðŸ”§ PHASE 2 â€” FINE-TUNE THE TOP CONVOLUTIONAL BLOCKS\n",
    "# ============================================================\n",
    "\n",
    "for layer in base.layers[-30:]:  # unfreeze last 30 layers\n",
    "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"âœ… Model unfrozen and recompiled for fine-tuning.\")\n",
    "print(\"Trainable layers:\", sum(l.trainable for l in model.layers))\n",
    "\n",
    "FINE_TUNE_EPOCHS = 5\n",
    "total_epochs = EPOCHS + FINE_TUNE_EPOCHS\n",
    "\n",
    "history_ft = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=EPOCHS\n",
    ")\n",
    "\n",
    "fine_tuned_path = models_dir / \"plant_classifier_v1_finetuned\"\n",
    "model.save(fine_tuned_path)\n",
    "print(\"âœ… Fine-tuned model saved at:\", fine_tuned_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a01d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Export to TFLite ----\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(str(fine_tuned_path))\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open(models_dir / \"plant_classifier_int8.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "print(\"âœ… Exported quantized TFLite model successfully\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
