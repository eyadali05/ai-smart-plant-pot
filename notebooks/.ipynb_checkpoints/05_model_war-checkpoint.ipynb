{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6f60a8-e6c7-4aa3-ac07-68ef4bcf92b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Imports\n",
    "# !pip install -U tensorflow pillow opencv-python albumentations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0b99f9-36f7-4f3f-907f-aa6d19391054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: 47 ['african_violet_saintpaulia_ionantha', 'aloe_vera', 'anthurium_anthurium_andraeanum', 'areca_palm_dypsis_lutescens', 'asparagus_fern_asparagus_setaceus', 'begonia_begonia_spp', 'bird_of_paradise_strelitzia_reginae', 'birds_nest_fern_asplenium_nidus', 'boston_fern_nephrolepis_exaltata', 'calathea', 'cast_iron_plant_aspidistra_elatior', 'chinese_evergreen_aglaonema', 'chinese_money_plant_pilea_peperomioides', 'christmas_cactus_schlumbergera_bridgesii', 'chrysanthemum', 'ctenanthe', 'daffodils_narcissus_spp', 'dracaena', 'dumb_cane_dieffenbachia_spp', 'elephant_ear_alocasia_spp', 'english_ivy_hedera_helix', 'hyacinth_hyacinthus_orientalis', 'iron_cross_begonia_begonia_masoniana', 'jade_plant_crassula_ovata', 'kalanchoe', 'lilium_hemerocallis', 'lily_of_the_valley_convallaria_majalis', 'money_tree_pachira_aquatica', 'monstera_deliciosa_monstera_deliciosa', 'orchid', 'parlor_palm_chamaedorea_elegans', 'peace_lily', 'poinsettia_euphorbia_pulcherrima', 'polka_dot_plant_hypoestes_phyllostachya', 'ponytail_palm_beaucarnea_recurvata', 'pothos_ivy_arum', 'prayer_plant_maranta_leuconeura', 'rattlesnake_plant_calathea_lancifolia', 'rubber_plant_ficus_elastica', 'sago_palm_cycas_revoluta', 'schefflera', 'snake_plant_sanseviera', 'tradescantia', 'tulip', 'venus_flytrap', 'yucca', 'zz_plant_zamioculcas_zamiifolia']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - paths and constants\n",
    "ROOT = Path.cwd().parent  # adjust if notebook is in a subfolder\n",
    "processed_root = ROOT / \"data_processed\"  # same structure as your uploaded notebooks\n",
    "models_dir = ROOT / \"models\"\n",
    "augment_samples_dir = models_dir / \"augment_samples\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "augment_samples_dir.mkdir(exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# get classes\n",
    "classes = sorted([p.name for p in processed_root.iterdir() if p.is_dir()])\n",
    "num_classes = len(classes)\n",
    "print(\"Classes found:\", num_classes, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b3031f-7971-4491-a6b5-05c579cb6875",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m         bad_files\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mstr\u001b[39m(f), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_image_ok\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# try convert clean\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m, in \u001b[0;36mis_image_ok\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_image_ok\u001b[39m(path):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         img\u001b[38;5;241m.\u001b[39mverify()\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\workspace\\AI F.R.I.E.N.D.S\\plantenv\\lib\\site-packages\\PIL\\Image.py:3493\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[0;32m   3492\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[1;32m-> 3493\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3494\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 3 - dataset cleaning (quick)\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "def is_image_ok(path):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        img.verify()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def convert_to_jpeg(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        new_path = path.with_suffix(\".jpg\")\n",
    "        img.save(new_path, \"JPEG\", quality=95)\n",
    "        if new_path != path:\n",
    "            os.remove(path)\n",
    "        return new_path\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# run cleaning\n",
    "bad_files = []\n",
    "converted = 0\n",
    "checked = 0\n",
    "for cls in classes:\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        folder = processed_root / cls / split\n",
    "        if not folder.exists():\n",
    "            continue\n",
    "        for f in list(folder.iterdir()):\n",
    "            if not f.is_file():\n",
    "                continue\n",
    "            checked += 1\n",
    "            if f.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
    "                new = convert_to_jpeg(f)\n",
    "                if new:\n",
    "                    converted += 1\n",
    "                else:\n",
    "                    bad_files.append((str(f), \"unsupported\"))\n",
    "                    continue\n",
    "            if not is_image_ok(f):\n",
    "                # try convert clean\n",
    "                try:\n",
    "                    img = Image.open(f).convert(\"RGB\")\n",
    "                    img.save(f, \"JPEG\", quality=90)\n",
    "                except Exception:\n",
    "                    bad_files.append((str(f), \"corrupt\"))\n",
    "                    try:\n",
    "                        os.remove(f)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "print(\"Checked:\", checked, \"Converted:\", converted, \"Bad found:\", len(bad_files))\n",
    "for p, reason in bad_files[:10]:\n",
    "    print(\"Bad:\", p, reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a2516-70bf-40fb-aec3-bf644ef78747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - augmentation function\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "def augment_numpy(image_np, rng=None):\n",
    "    \"\"\"\n",
    "    image_np: uint8 HWC (0..255)\n",
    "    returns augmented uint8 HWC\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    img = Image.fromarray(image_np)\n",
    "\n",
    "    # Random close crop (simulate zoom/partial plant framing)\n",
    "    if rng.random() < 0.8:\n",
    "        w, h = img.size\n",
    "        scale = rng.uniform(0.85, 1.0)\n",
    "        new_w, new_h = int(w*scale), int(h*scale)\n",
    "        left = rng.integers(0, max(1, w-new_w))\n",
    "        top = rng.integers(0, max(1, h-new_h))\n",
    "        img = img.crop((left, top, left+new_w, top+new_h))\n",
    "        img = img.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n",
    "    else:\n",
    "        img = img.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n",
    "\n",
    "    # Brightness & contrast\n",
    "    if rng.random() < 0.6:\n",
    "        img = ImageEnhance.Brightness(img).enhance(rng.uniform(0.7, 1.25))\n",
    "    if rng.random() < 0.6:\n",
    "        img = ImageEnhance.Contrast(img).enhance(rng.uniform(0.7, 1.25))\n",
    "\n",
    "    # Slight green tint (Pi camera bias)\n",
    "    if rng.random() < 0.5:\n",
    "        arr = np.array(img).astype(np.float32)\n",
    "        arr[:,:,1] = np.clip(arr[:,:,1] * rng.uniform(0.98, 1.06), 0, 255)\n",
    "        img = Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "    # JPEG compression artifacts\n",
    "    if rng.random() < 0.5:\n",
    "        q = int(rng.uniform(35, 85))\n",
    "        buff = BytesIO()\n",
    "        img.save(buff, format='JPEG', quality=q)\n",
    "        buff.seek(0)\n",
    "        img = Image.open(buff).convert('RGB')\n",
    "\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "\n",
    "    # Gaussian noise\n",
    "    if rng.random() < 0.5:\n",
    "        noise = rng.normal(0, rng.uniform(5, 20), arr.shape)\n",
    "        arr = np.clip(arr + noise, 0, 255)\n",
    "\n",
    "    # Motion blur\n",
    "    if rng.random() < 0.35:\n",
    "        k = int(rng.integers(3, 9))\n",
    "        kernel = np.zeros((k, k))\n",
    "        if rng.random() < 0.5:\n",
    "            kernel[k//2, :] = 1.0 / k\n",
    "        else:\n",
    "            kernel[:, k//2] = 1.0 / k\n",
    "        arr = cv2.filter2D(arr.astype(np.uint8), -1, kernel)\n",
    "\n",
    "    # Vignette / shadow\n",
    "    if rng.random() < 0.35:\n",
    "        h, w, _ = arr.shape\n",
    "        Y, X = np.ogrid[:h, :w]\n",
    "        dist = np.sqrt((X - w/2)**2 + (Y - h/2)**2)\n",
    "        mask = 1 - 0.6 * (dist / np.sqrt(w*w + h*h))\n",
    "        mask = np.clip(mask, 0.4, 1).astype(np.float32)\n",
    "        arr = (arr * mask[..., None])\n",
    "\n",
    "    # Small median smoothing sometimes\n",
    "    if rng.random() < 0.25:\n",
    "        arr = cv2.medianBlur(arr.astype(np.uint8), 3)\n",
    "\n",
    "    out = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ce870-a0ad-4bc2-8ad3-b2fbc64b463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - tf wrappers for loading and augment (SAFE VERSION for TF 2.12)\n",
    "\n",
    "def load_image_simple(path, label):\n",
    "    \"\"\"Load image, resize, return float32 [0,1]; skip corrupt ones\"\"\"\n",
    "    def _load(p, lbl):\n",
    "        try:\n",
    "            path_str = p.decode() if isinstance(p, bytes) else str(p)\n",
    "            img = Image.open(path_str).convert(\"RGB\")\n",
    "            img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "            arr = np.array(img, dtype=np.float32) / 255.0\n",
    "            return arr, np.int32(lbl)\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ Skipping bad file:\", p, e)\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), np.float32), np.int32(-1)\n",
    "    img, lbl = tf.py_function(_load, [path, label], (tf.float32, tf.int32))\n",
    "    img.set_shape([IMG_SIZE, IMG_SIZE, 3])\n",
    "    lbl.set_shape([])\n",
    "    return img, lbl\n",
    "\n",
    "\n",
    "def load_and_augment_train(path, label):\n",
    "    \"\"\"Load and apply augment_numpy via py_function, skipping bad files\"\"\"\n",
    "    def _process(p, lbl):\n",
    "        try:\n",
    "            path_str = p.decode() if isinstance(p, bytes) else str(p)\n",
    "            img = Image.open(path_str).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "            arr = np.array(img, dtype=np.uint8)\n",
    "            arr = augment_numpy(arr)\n",
    "            arr = arr.astype(np.float32) / 255.0\n",
    "            return arr, np.int32(lbl)\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ Skipping bad file:\", p, e)\n",
    "            return np.zeros((IMG_SIZE, IMG_SIZE, 3), np.float32), np.int32(-1)\n",
    "    img, lbl = tf.py_function(_process, [path, label], (tf.float32, tf.int32))\n",
    "    img.set_shape([IMG_SIZE, IMG_SIZE, 3])\n",
    "    lbl.set_shape([])\n",
    "    return img, lbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca4932-1472-48d1-b3d9-a650db641adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - build path lists and datasets (SAFE VERSION)\n",
    "\n",
    "def get_paths_and_labels(class_names, split):\n",
    "    paths, labels = [], []\n",
    "    for idx, cls in enumerate(class_names):\n",
    "        p = processed_root / cls / split\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        for f in p.iterdir():\n",
    "            if f.is_file():\n",
    "                paths.append(str(f))\n",
    "                labels.append(idx)\n",
    "    return paths, labels\n",
    "\n",
    "\n",
    "train_paths, train_labels = get_paths_and_labels(classes, \"train\")\n",
    "val_paths, val_labels = get_paths_and_labels(classes, \"val\")\n",
    "test_paths, test_labels = get_paths_and_labels(classes, \"test\")\n",
    "\n",
    "print(\"Train/Val/Test sizes:\", len(train_paths), len(val_paths), len(test_paths))\n",
    "\n",
    "# ✅ Safe tf.data pipelines\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "    .shuffle(4000, reshuffle_each_iteration=True)\n",
    "    .map(load_and_augment_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .filter(lambda img, lbl: tf.not_equal(lbl, -1))\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)   # ensures full, non-empty batches\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "    .map(load_image_simple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .filter(lambda img, lbl: tf.not_equal(lbl, -1))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "    .map(load_image_simple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .filter(lambda img, lbl: tf.not_equal(lbl, -1))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a79054-0785-4ae1-8a40-a40b2115900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - save sample images (before and after augmentation)\n",
    "import random\n",
    "from pathlib import Path\n",
    "sample_dir = augment_samples_dir / \"sample_images\"\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def save_random_originals(n=8):\n",
    "    picked = random.sample(train_paths, min(n, len(train_paths)))\n",
    "    for i, p in enumerate(picked):\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "            img.save(sample_dir / f\"orig_{i:02d}.jpg\")\n",
    "        except:\n",
    "            pass\n",
    "    print(\"Saved originals to:\", sample_dir)\n",
    "\n",
    "def save_augmented_from_pipeline(n=8):\n",
    "    # take batches from train_ds (which yields augmented images)\n",
    "    i = 0\n",
    "    for batch in train_ds:\n",
    "        imgs, labels = batch\n",
    "        for j in range(imgs.shape[0]):\n",
    "            arr = (imgs[j].numpy() * 255).astype(np.uint8)\n",
    "            Image.fromarray(arr).save(sample_dir / f\"aug_{i:03d}.jpg\")\n",
    "            i += 1\n",
    "            if i >= n:\n",
    "                print(\"Saved augmented samples to:\", sample_dir)\n",
    "                return\n",
    "\n",
    "# Run them\n",
    "save_random_originals(8)\n",
    "save_augmented_from_pipeline(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdea9a8-feff-4e64-8fb8-0b7bca464654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 - show a few saved samples\n",
    "from IPython.display import display\n",
    "files = sorted(list(sample_dir.glob(\"*.jpg\")))\n",
    "for f in files[:8]:\n",
    "    display(Image.open(f).resize((200,200)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7529510-9431-477c-ac49-1ae904f66fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 - build model\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Option A (recommended): MobileNetV3Small (fast + small)\n",
    "base = tf.keras.applications.MobileNetV3Small(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    minimalistic=True\n",
    ")\n",
    "\n",
    "# Alternative Option B (if you prefer MobileNetV2 as used in 03):\n",
    "# base = tf.keras.applications.MobileNetV2(input_shape=(IMG_SIZE,IMG_SIZE,3), include_top=False, weights='imagenet')\n",
    "\n",
    "base.trainable = False  # freeze base initially\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = base(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97d21d-b1bd-4d4f-acd5-a4a04112023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 - training stage 1 (train head, stable version)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "\n",
    "EPOCHS_HEAD = 10\n",
    "\n",
    "lr_schedule = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-7, verbose=1\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "class SaveAugmentSamples(Callback):\n",
    "    \"\"\"Saves one augmented sample per epoch (optional visual check)\"\"\"\n",
    "    def __init__(self, out_dir, sample_path):\n",
    "        super().__init__()\n",
    "        self.out_dir = Path(out_dir)\n",
    "        self.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.sample_path = sample_path\n",
    "        self.original = Image.open(sample_path).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        aug_np = augment_numpy(np.array(self.original))\n",
    "        Image.fromarray(aug_np).save(self.out_dir / f\"epoch_{epoch+1:02d}_aug.jpg\")\n",
    "\n",
    "sample_val = val_paths[0] if len(val_paths) > 0 else train_paths[0]\n",
    "save_aug_cb = SaveAugmentSamples(out_dir=augment_samples_dir / \"epochs\", sample_path=sample_val)\n",
    "\n",
    "# ✅ Re-compile to clear any graph cache\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# ✅ Start training (this will now run cleanly)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS_HEAD,\n",
    "    callbacks=[lr_schedule, early_stop, save_aug_cb],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa62d195-c808-4628-8ced-6a7e410299e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 - fine-tuning\n",
    "base.trainable = True\n",
    "# Optionally unfreeze only top layers:\n",
    "# for layer in base.layers[:-30]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452854d6-ab39-4419-ac93-4c19d2f52268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 - evaluation & save TF model\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=1)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "tf_model_path = models_dir / \"plant_classifier_tf\"\n",
    "model.save(tf_model_path)\n",
    "print(\"Saved Keras model to:\", tf_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce581f-77fa-483d-9f26-358d0443b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 - TFLite INT8 quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Full integer quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Ensure input/output are int8\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "# Representative dataset: yield 1 sample at a time as a numpy array scaled to uint8 if needed\n",
    "def representative_dataset_gen():\n",
    "    for image_batch, _ in val_ds.take(100):\n",
    "        # image_batch is float32 [0,1], shape [B, H, W, C]\n",
    "        # convert to uint8 [0,255]\n",
    "        img_uint8 = (image_batch * 255.0).numpy().astype(np.uint8)\n",
    "        for i in range(img_uint8.shape[0]):\n",
    "            yield [img_uint8[i:i+1]]\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "tflite_path = models_dir / \"plant_classifier_int8.tflite\"\n",
    "tflite_path.write_bytes(tflite_model)\n",
    "print(\"Saved quantized TFLite to:\", tflite_path, \"size KB:\", len(tflite_model)/1024)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
